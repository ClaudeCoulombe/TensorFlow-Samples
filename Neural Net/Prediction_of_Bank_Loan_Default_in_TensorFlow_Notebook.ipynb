{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Bank Loan Default in TensorFlow\n",
    "## inspired by \"A Unique TensorFlow Neural Net Tutorial using a Jupyter Python Notebook\" \n",
    "### in IBM Data Science Experience\n",
    "### by John M. Boyer \n",
    "\n",
    "Web site: https://www.ibm.com/developerworks/community/blogs/JohnBoyer\n",
    "\n",
    "GitHub code repo: https://github.com/john-boyer-phd/TensorFlow-Samples\n",
    "\n",
    "This TensorFlow Neural Network tutorial has several aspects that are unique or not evident in other tutorials like the MNIST handwritten digits tutorial. \n",
    "\n",
    "The focus is on business, both in terms of the use case, prediction of bank loan risk, and data and in terms of extra steps needed to help take your data science results to production. \n",
    "\n",
    "We’ll cover the following:\n",
    "\n",
    "* Reading data and reshaping it for TensorFlow neural net input\n",
    "* Epoch based training and training data randomization\n",
    "* Training in small batches for larger data sets\n",
    "* Tuning hyperparameters like the network structure and activation function\n",
    "* Tricks for properly saving and restoring models for use in a production environment\n",
    "* How to do transfer learning\n",
    "* How to derive confidence values for neural net outputs \n",
    "\n",
    "#### Note: \n",
    "\n",
    "This derived Notebook is a small contribution by Claude Coulombe, PhD candidate, TÉLUQ / UQAM\n",
    "\n",
    "This is not a deep neural network, in fact it's a pretty shallow one with just an unique hidden layer. It is more an exercise in TensorFlow to show how a simple neural network can be used to predict a loan default using a simple binary classifier. Claude Coulombe, PhD candidate - Téluq / UQAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem - prediction of loan default\n",
    "\n",
    "Default is the failure to pay interest or principal on a loan when due. For example when a home buyer fails to make a mortgage payment. \n",
    "\n",
    "The goal of this small exercise will be to train and perform inferences with a `TensorFlow` neural network for predicting whether a loan applicant is likely to `default` on a bank loan, based on features of the applicant that may be predictive of their ability to repay a loan. <u>The dependent variable being predicted is the column named ‘default’</u> in the CSV file. The dependent variable is also called the `label` or `target`, and the data in the column is called the `labeled data`. The predictor variables are `age`, `ed` (level of education), `employ` (years with current employer), `address` (years at current address), `income` (household income in thousands), `debtinc` (debt to income ratio x 100), `creddebt` (credit card debt in thousands), and `othdebt` (other debt in thousands). The predictor variables are also called `features`.  \n",
    "\n",
    "The remaining columns of data are not needed and will be discarded in the code below. When training, the feature values of the instances (rows) of data are fed as input to the neural net, and the weights and biases of the neural network are adjusted so as to minimize `loss`, which coarsely maps to maximizing accuracy of the neural network’s output layer predictions of the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The sample data we’ll be using for training and testing is in a file called `bankLoanData.csv`, which is a sample data file I obtained from my laptop IBM SPSS statistics package. I’ve used this data because I could easily use SPSS to double-check that all the `TensorFlow` code was behaving as I expected. The advantage to both you and me, then, is that we can now easily adapt the resulting `TensorFlow` code to build bigger neural nets that learn from much larger data sets.\n",
    "\n",
    "The first cell of the Jupyter iPython Notebook below has to do some version of reading the CSV file. In my prior tutorial, I showed how to load a CSV file into a database and then load the data into a Pandas dataframe using a SQL query. For this tutorial, any version of `pandas.read_csv()` will suffice. For example, in IBM Data Science Experience on IBM Cloud, you can simply drag and drop the CSV file to add it as a dataset, and then select “Insert Code” to automatically generate the code to read the CSV file from cloud object storage. For larger datasets, you may prefer to use a `SparkSession Dataframe` instead, but in that case, you’ll need to slightly adjust the `numpy` extraction code in the next Notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "      <th>preddef1</th>\n",
       "      <th>preddef2</th>\n",
       "      <th>preddef3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808394</td>\n",
       "      <td>0.788640</td>\n",
       "      <td>0.213043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198297</td>\n",
       "      <td>0.128445</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.141023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.104422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>0.737885</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  ed  employ  address  income  debtinc   creddebt   othdebt default  \\\n",
       "0   41   3      17       12     176      9.3  11.359392  5.008608       1   \n",
       "1   27   1      10        6      31     17.3   1.362202  4.000798       0   \n",
       "2   40   1      15       14      55      5.5   0.856075  2.168925       0   \n",
       "3   41   1      15       14     120      2.9   2.658720  0.821280       0   \n",
       "4   24   2       2        0      28     17.3   1.787436  3.056564       1   \n",
       "\n",
       "   preddef1  preddef2  preddef3  \n",
       "0  0.808394  0.788640  0.213043  \n",
       "1  0.198297  0.128445  0.436903  \n",
       "2  0.010036  0.002987  0.141023  \n",
       "3  0.022138  0.010273  0.104422  \n",
       "4  0.781588  0.737885  0.436903  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_data_1 = pd.read_csv('bankloanData.csv')\n",
    "df_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell of code below assumes a Pandas dataframe named ‘df_data_1’ and uses it to extract the data into numpy arrays needed as input to the TensorFlow API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           int64\n",
       "ed            int64\n",
       "employ        int64\n",
       "address       int64\n",
       "income        int64\n",
       "debtinc     float64\n",
       "creddebt    float64\n",
       "othdebt     float64\n",
       "default      object\n",
       "preddef1    float64\n",
       "preddef2    float64\n",
       "preddef3    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 3, 17, ..., 0.808394327359702, 0.7886404318214371,\n",
       "        0.21304337612811897],\n",
       "       [27, 1, 10, ..., 0.19829747615910395, 0.128445387038174,\n",
       "        0.43690300550604605],\n",
       "       [40, 1, 15, ..., 0.0100361080990023, 0.00298677834821412,\n",
       "        0.141022623460993],\n",
       "       ..., \n",
       "       [48, 1, 13, ..., 0.0301374981044824, 0.0325702625943738,\n",
       "        0.24801041775523303],\n",
       "       [35, 2, 1, ..., 0.26900345101699397, 0.37854649636973203,\n",
       "        0.181814378077261],\n",
       "       [37, 1, 20, ..., 0.006397812918809229, 0.0111731232851226,\n",
       "        0.30304155578236497]], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comprehension in the first np.array() removes instances (rows) that have a missing `default` label.\n",
    "\n",
    "The eight (8) predictor variables are `age`, `ed` (level of education), `employ` (years with current employer), `address` (years at current address), `income` (household income in thousands), `debtinc` (debt to income ratio x 100), `creddebt` (credit card debt in thousands), and `othdebt` (other debt in thousands). The remaining columns of data are not needed and will be discarded in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a numpy array from the dataframe, except remove rows with no value for 'default'\n",
    "i = list(df_data_1.columns.values).index('default')\n",
    "data = np.array([x for x in df_data_1.values if x[i] in ['0', '1']])\n",
    "\n",
    "# Remove the columns for preddef1, predef2 and preddef3\n",
    "data = np.delete(data, slice(9,12), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell reshapes the data just a bit. The predictors are separated from the dependent variable. The labeled data is converted from strings to integers, and the dependent array is flattened to one dimension (from one column matrix to a one line matrix) to match the shape of the data that will come from the neural network output layer (one line matrix).  The predictors are converted to all floats to facilitate matrix multiplication with weights and biases within the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 1)\n",
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]] \n",
      "  .\n",
      "  .\n",
      "  .\n",
      "\n",
      "(700,)\n",
      "[1 0 0 0 1 0 0 0 1 0] ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(700, 8)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the 'predictors' (aka 'features') from the dependent variable (aka 'label' or 'target') \n",
    "# that we will learn how to predict\n",
    "predictors = np.delete(data, 8, axis=1)\n",
    "dependent = np.delete(data, slice(0, 8), axis=1)\n",
    "\n",
    "# Convert the label (aka 'target' type to numeric categorical \n",
    "# representing the classes to predict (binary classifier)\n",
    "dependent = dependent.astype(int)\n",
    "print(dependent.shape)\n",
    "print(dependent[:10],\"\\n  .\\n  .\\n  .\\n\")\n",
    "\n",
    "# And flatten it to one dimensional for use as the expected output label vector in TensorFlow\n",
    "dependent = dependent.flatten()\n",
    "print(dependent.shape)\n",
    "print(dependent[:10],\"...\")\n",
    "\n",
    "# Convert all the predictors to float to simplify this demo TensorFlow code\n",
    "predictors = predictors.astype(float)\n",
    "\n",
    "# Get the shape of the predictors\n",
    "m, n = predictors.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.      ,    3.      ,   17.      ,   12.      ,  176.      ,\n",
       "          9.3     ,   11.359392,    5.008608])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell simply takes the first 500 instances as training data, leaving the remaining 200 instances for a test set. It’s not unusual to randomly select the training and test sets from the given data, but this particular sample was already random.  It’s also typical to choose about a 70/30 percent split for training and test, and this code does so, except for rounding to a size divisible by the training batch size we’ll define later (700*0.7 => 489.999 rounding => 500). \n",
    "\n",
    "This cell also defines a method that returns batch-sized slices of the training data. If the training data were too large to fit in memory, then this method could instead load data one batch at a time, such as with a SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Partition the input data into a training set and a test set\n",
    "\n",
    "m_train = 500\n",
    "m_test = m - m_train\n",
    "\n",
    "predictors_train = predictors[:m_train]\n",
    "dependent_train = dependent[:m_train]\n",
    "\n",
    "predictors_test = predictors[m_train:]\n",
    "dependent_test = dependent[m_train:]\n",
    "\n",
    "# Gets a batch of the training data. \n",
    "# NOTE: Rather than loading a whole large data set as above and then taking array slices as done here, \n",
    "#       This method can connect to a data source and select just the batch needed.\n",
    "def get_training_batch(batch_num, batch_size):\n",
    "    lower = batch_num * (m_train // batch_size)\n",
    "    upper = lower + batch_size\n",
    "    return predictors_train[lower:upper], dependent_train[lower:upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up the neural network using TensorFlow\n",
    "\n",
    "Now we’re set to start with some actual TensorFlow code. This next cell imports TensorFlow, makes a few useful initializations, and then defines a method that will build a neural network layer of a given size, fully connect it to a preceding layer, and set its output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Make this notebook's output stable across runs \n",
    "# ensure reproducibility of the results\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# A method to build a new neural net layer of a given size,  \n",
    "# fully connect it to a given preceding layer X, and \n",
    "# compute its output Z either with or without (default) an activation function\n",
    "# Call with activation=tf.nn.relu or tf.nn.sigmoid or tf.nn.tanh, for examples\n",
    "\n",
    "def make_nn_layer(layer_name, layer_size, X, activation=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        X_size = int(X.get_shape()[1])\n",
    "        SD = 2 / np.sqrt(X_size)\n",
    "        weights = tf.truncated_normal((X_size, layer_size), dtype=tf.float64, stddev=SD)\n",
    "        W = tf.Variable(weights, name='weights')\n",
    "        b = tf.Variable(tf.zeros([layer_size], dtype=tf.float64), name='biases')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add the code cell that builds the neural network structure. In this case, we’re going to have one input layer (X), one hidden layer (hidden1), and one output layer (outputs). \n",
    "\n",
    "The ### n_hidden2 = n // 2 comments show how to add more hidden layers, but with this sample data, we’re going to be able to learn everything we can with only one layer. The output layer has two nodes, one for outputting class 0 (the loan applicant won’t default) and the other for class 1 (the loan applicant will default). The ‘y’ variable will be used during training to store the labeled data we expect to match with the output layer.\n",
    "\n",
    "One line of code that helps makes this tutorial unique is the one that creates a tf.identity() node that gives the name ‘nn_output’. This enables us to save a name for the output layer so that we can recover and use the output layer after a restore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the neural net structure\n",
    "\n",
    "n_inputs = n\n",
    "n_hidden1 = n \n",
    "### n_hidden2 = n // 2\n",
    "n_outputs = 2   # Two output classes: defaulting or non-defaulting on loan\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=(None, n_inputs), name='X')\n",
    "\n",
    "with tf.name_scope('nn'):\n",
    "    hidden1 = make_nn_layer('hidden1', n_hidden1, X, activation=tf.nn.relu)\n",
    "    hidden2 = hidden1\n",
    "    ### hidden2 = make_nn_layer('hidden2', n_hidden2, hidden1, activation=tf.nn.relu)\n",
    "    outputs = make_nn_layer('outputs', n_outputs, hidden2) \n",
    "    outputs = tf.identity(outputs, \"nn_output\")\n",
    "    \n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above and the next cell are where most of the hyperparameter tuning occurs. Neural network is just the algorithm. The input parameters passed to a neural network during inference are the feature values. During training, the input parameters are feature values and the expected output labeled data. But the neural network is adaptable beyond those input parameters, and so these configurable parts are called hyperparameters. The number and size of the hidden layers are among the hyperparameters, as is the activation function.  For examples, you can try other numbers and sizes of hidden layers, and ‘tanh’ and ‘sigmoid’ are other activation functions to try. However, the given configuration seems to do very near the best on this data.\n",
    "\n",
    "What we’ve done so far is to create the main part of a TensorFlow compute graph that happens to have the shape needed for a neural network. \n",
    "\n",
    "What we’re going to do in the next cell below is attach two different root nodes to the output layer, one that adds functionality for training and the other for testing. The `training_op` uses the gradient descent method for minimizing loss (of perfect confidence in the correct answers and zero confidence in incorrect answers, where the correct answers are provided by the labeled data that will be in `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define how the neural net will learn\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "    loss = tf.reduce_mean(xentropy, name='l')\n",
    "    \n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test\"):\n",
    "    correct = tf.nn.in_top_k(tf.cast(outputs, tf.float32), y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re going to do a quick little cell that sets up our ability to save the model once it is trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the ability to save and restore the trained neural net...\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to do these mkdir commands the first time you run the notebook, so you may want to put them in a separate cell to make it easier to skip them. Also, in Data Science Experience Local, you only need the second mkdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../datasets: File exists\n",
      "mkdir: ../datasets/Neural Net: File exists\n"
     ]
    }
   ],
   "source": [
    "# ... and make a subdirectory space in which to save the model files (only need to run this once)\n",
    "!mkdir \"../datasets\"\n",
    "!mkdir \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training models\n",
    "\n",
    "Now we can have the magic notebook cell that trains and saves the trained model. Each epoch of training exposes the neural net to the entire set of training data. When you see this code run, you will see accuracy increase over the many epochs, just as biological neural networks learn through repetition. For each epoch, we run through the training data in batches, to simulate how we’d handle a larger training set. Each batch of features and corresponding labeled data is fed to the `training_op` root node in the compute graph, which is run by `training_session.run()`.\n",
    "\n",
    "One aspect of this tutorial that is made evident (relative to other tutorials) is the randomization of the training data that takes place at the beginning of each epoch.  This essentially drives different data into the batches in each epoch, which dramatically improves accuracy over a larger number of epochs (though it is much easier programmatically to do this randomization when all data fits into memory).\n",
    "\n",
    "> Yet another reason why this tutorial is unique is that we’ll actually take a little sidebar to understand why, when doing business with a real stakeholder customer, we need to have a second test set, often called a <u>validation set</u> or a blind set. Why do we need a second test set? When I ask this, the usual reply is something like, “I don’t know, to double-check accuracy?”  Well, sort of. But if you look at the structure of training, the weights and biases are affected not just by the training data. Indirectly, they are also affected by the test data because we choose `n_epochs` to keep running training epochs until we get the best accuracy on the test set. In other words, we’re teaching to the test. The validation set or blind set has no such indirect effect on the weights and biases computed for the neural network. It is simply another test set that, to ensure construct validity, should be randomly from the same pool of data that the training set and test set are randomly selected from. In this way, the validation set is not just the `final exam`, it’s the first experience of the real world. Sidebar complete.\n",
    "\n",
    "Once all training has been done, we save the trained model into the previously created datasets subdirectory. In this sample code, we are only saving at the end, but this same command can be used to save the intermediate results of a very long training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Training accuracy: 0.76 Testing accuracy: 0.755\n",
      "200 Training accuracy: 0.774 Testing accuracy: 0.79\n",
      "300 Training accuracy: 0.798 Testing accuracy: 0.82\n",
      "400 Training accuracy: 0.798 Testing accuracy: 0.835\n",
      "500 Training accuracy: 0.808 Testing accuracy: 0.825\n",
      "600 Training accuracy: 0.796 Testing accuracy: 0.815\n",
      "700 Training accuracy: 0.802 Testing accuracy: 0.81\n",
      "800 Training accuracy: 0.788 Testing accuracy: 0.815\n",
      "900 Training accuracy: 0.81 Testing accuracy: 0.815\n",
      "1000 Training accuracy: 0.82 Testing accuracy: 0.81\n",
      "1100 Training accuracy: 0.818 Testing accuracy: 0.81\n",
      "1200 Training accuracy: 0.806 Testing accuracy: 0.805\n",
      "1300 Training accuracy: 0.802 Testing accuracy: 0.815\n",
      "1400 Training accuracy: 0.812 Testing accuracy: 0.82\n",
      "1500 Training accuracy: 0.806 Testing accuracy: 0.8\n",
      "1600 Training accuracy: 0.816 Testing accuracy: 0.81\n",
      "1700 Training accuracy: 0.806 Testing accuracy: 0.81\n",
      "1800 Training accuracy: 0.822 Testing accuracy: 0.825\n",
      "1900 Training accuracy: 0.802 Testing accuracy: 0.805\n",
      "2000 Training accuracy: 0.83 Testing accuracy: 0.82\n",
      "2100 Training accuracy: 0.812 Testing accuracy: 0.81\n",
      "2200 Training accuracy: 0.824 Testing accuracy: 0.825\n",
      "2300 Training accuracy: 0.804 Testing accuracy: 0.815\n",
      "2400 Training accuracy: 0.818 Testing accuracy: 0.815\n",
      "2500 Training accuracy: 0.828 Testing accuracy: 0.825\n",
      "2600 Training accuracy: 0.83 Testing accuracy: 0.815\n",
      "2700 Training accuracy: 0.822 Testing accuracy: 0.815\n",
      "2800 Training accuracy: 0.82 Testing accuracy: 0.805\n",
      "2900 Training accuracy: 0.814 Testing accuracy: 0.83\n",
      "3000 Training accuracy: 0.826 Testing accuracy: 0.825\n",
      "\n",
      "Actual classes:    [0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      "Predicted classes: [1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# TRAINING TIME\n",
    "\n",
    "# This is how many times to use the full set of training data\n",
    "n_epochs = 3000\n",
    "\n",
    "# For a larger training set, it's typically necessary to break training into\n",
    "# batches so only the memory needed to store one batch of training data is used\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as training_session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Shuffling (across batches) is easier to do for small data sets and\n",
    "        # helps increase accuracy\n",
    "        training_set = [[pt_elem, dependent_train[i]] for i, pt_elem in enumerate(predictors_train)]\n",
    "        np.random.shuffle(training_set)\n",
    "        predictors_train = [ts_elem[0] for ts_elem in training_set]\n",
    "        dependent_train = [ts_elem[1] for ts_elem in training_set]\n",
    "       \n",
    "        # Loop through the whole training set in batches\n",
    "        for batch_num in range(m_train // batch_size):\n",
    "            X_batch, y_batch = get_training_batch(batch_num, batch_size)\n",
    "            training_session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        if epoch % 100 == 99:\n",
    "            acc_train = accuracy.eval(feed_dict={X: predictors_train, y: dependent_train})\n",
    "            acc_test = accuracy.eval(feed_dict={X: predictors_test, y: dependent_test})\n",
    "            print(epoch+1, \"Training accuracy:\", acc_train, \"Testing accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(training_session, \"../datasets/Neural Net/Neural Net.ckpt\")\n",
    "    \n",
    "    # A quick test with the trained model \n",
    "    Z = outputs.eval(feed_dict={X: predictors_test[:20]})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    print(\"\")\n",
    "    print(\"Actual classes:   \", dependent_test[:20])  \n",
    "    print(\"Predicted classes:\", dependent_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files that TensorFlow created during the save operation can be transported to a production environment. The neural network can then be restored using the code in the next cell below, and the output layer can be obtained and used for inference (using `get_tensor_by_name()`).  In fact, showing how to do that is part of what makes this tutorial unique, as even the current TensorFlow documentation for save/restore (incorrectly) reuses variables after restore that were defined before save (rather than running variables obtained from the restored graph). The code below also shows how to reference into the hierarchy of a namescope. \n",
    "\n",
    "> As another sidebar unique in this tutorial, note that you can also use this method of naming with `tf.identity` and then getting the tensor from a restored graph to do transfer learning between neural nets. Specifically, once you create a hidden layer with `make_nn_layer()`, you can name it with `tf.identity`. Then, you train and save as shown above. Then, to transfer to a second neural net, you restore the trained and saved one, get the hidden layer by name, attach alternate fully connected hidden layers as needed, and an alternate output layer, and then train the new second neural network using the methodology above. Sidebar complete.\n",
    "\n",
    "The cell below mocks up having a REST API receiving a batch of feature instances and converting them to a numpy array by simply taking a slice of the test data.  With the inference TensorFlow session, the compute graph and the values it contained are restored. After that, we obtain the tensor corresponding to the neural network output layer by using the name we previously assigned.  Then, we run the output layer, giving the batch of feature instances to the input layer ‘X’ (`inference_session.run()`). The predictions of the dependent variable are then obtained by choosing whichever of the two output layer nodes has the higher value (using `np.argmax()`).\n",
    "\n",
    "Finally, to add one more feature that makes this tutorial unique, let’s look at how to get the actual confidence values for the predictions. Somehow, this may not have seemed as important when doing the MNIST hand-written digit tutorial, but in a business context it’s important to know how much confidence we have in an answer. \n",
    "\n",
    "To get the confidence values, we do something interesting with the compute graph. Remember, it’s just a compute graph and won’t bite. In this case, we pop a new root node onto the output layer to apply the softmax function, which gives the probability of occurrence of each output value. Then, we do one last comprehension to ferret out the confidences of the predicted labels for each feature instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../datasets/Neural Net/Neural Net.ckpt\n",
      "\n",
      "Actual classes:    [0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1]\n",
      "Predicted classes: [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1]\n",
      "\n",
      "Confidences:\n",
      " [0.72115788841227302, 0.80607047578305713, 0.83934041973914708, 0.99522251895376912, 0.63583707224033514, 0.92301370061024191, 0.9246727235093638, 0.78439759502501416, 0.99999999999583422, 0.64561868763996888, 0.72736408931601459, 0.64561868763996888, 0.83959328566375235, 0.94291801514712859, 0.68170960394665236, 0.99981345698850999, 0.96778595641239296, 0.59248611260528017, 0.99293831799558985, 0.52204835458766652]\n",
      "\n",
      "Probabilities:\n",
      " [[  7.21157888e-01   2.78842112e-01]\n",
      " [  8.06070476e-01   1.93929524e-01]\n",
      " [  8.39340420e-01   1.60659580e-01]\n",
      " [  9.95222519e-01   4.77748105e-03]\n",
      " [  6.35837072e-01   3.64162928e-01]\n",
      " [  9.23013701e-01   7.69862994e-02]\n",
      " [  9.24672724e-01   7.53272765e-02]\n",
      " [  7.84397595e-01   2.15602405e-01]\n",
      " [  1.00000000e+00   4.16586334e-12]\n",
      " [  3.54381312e-01   6.45618688e-01]\n",
      " [  7.27364089e-01   2.72635911e-01]\n",
      " [  3.54381312e-01   6.45618688e-01]\n",
      " [  8.39593286e-01   1.60406714e-01]\n",
      " [  9.42918015e-01   5.70819849e-02]\n",
      " [  6.81709604e-01   3.18290396e-01]\n",
      " [  9.99813457e-01   1.86543011e-04]\n",
      " [  9.67785956e-01   3.22140436e-02]\n",
      " [  4.07513887e-01   5.92486113e-01]\n",
      " [  9.92938318e-01   7.06168200e-03]\n",
      " [  4.77951645e-01   5.22048355e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Restore the saved model and use it to perform inference on a \"received\" new set of data\n",
    "\n",
    "# We will simulate \"receiving\" the new data by taking a slice of the test set.\n",
    "predictors_received = predictors_test[20:40]\n",
    "\n",
    "import tensorflow as tf_inference\n",
    "\n",
    "with tf_inference.Session() as inference_session:\n",
    "    inf_saver = tf_inference.train.import_meta_graph('../datasets/Neural Net/Neural Net.ckpt.meta')\n",
    "    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../datasets/Neural Net/'))\n",
    "    \n",
    "    graph = tf_inference.get_default_graph()    \n",
    "    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n",
    "\n",
    "    Z = inference_session.run(nn_output, feed_dict={X: predictors_received})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "    dependent_prob = inference_session.run(tf_inference.nn.softmax(nn_output), feed_dict={X: predictors_received})\n",
    "\n",
    "    confidences = [p[dependent_pred[i]] for i, p in enumerate(dependent_prob)]\n",
    "    \n",
    "print()\n",
    "print(\"Actual classes:   \", dependent_test[20:40])\n",
    "print(\"Predicted classes:\", dependent_pred)\n",
    "print(\"\")\n",
    "print(\"Confidences:\\n\", confidences)\n",
    "print(\"\")\n",
    "print(\"Probabilities:\\n\", dependent_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For when you want to wipe out the training and do it again\n",
    "!rm -rf \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: ../datasets/Neural Net: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 32\r\n",
      "-rw-r--r--  1 claudecoulombe  staff  11357 11 nov 03:06 LICENSE\r\n",
      "drwxr-xr-x  5 claudecoulombe  staff    170 12 nov 01:10 \u001b[34mNeural Net\u001b[m\u001b[m\r\n",
      "-rw-r--r--  1 claudecoulombe  staff    252 11 nov 03:06 README.md\r\n",
      "drwxr-xr-x  2 claudecoulombe  staff     68 12 nov 01:12 \u001b[34mdatasets\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l \"..\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf \"../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And that’s a wrap. Now, it’s your turn. Go ahead, get started with that free IBM Data Science Experience account so you can amaze your friends and bosses with your newfound TensorFlow AI machine learning data science hyperparametric hyperwizardry. You know you waaaanna!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
